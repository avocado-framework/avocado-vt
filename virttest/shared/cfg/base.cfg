# Copy this file to base.cfg and edit it

# Absolute paths and/or names of binaries (default path is /usr/bin)
qemu_binary = qemu
qemu_img_binary = qemu-img
qemu_io_binary = qemu-io

# Qemu cmd prefix (to attach qemu to valgrind, for example)
#qemu_command_prefix = valgrind

# Explicitly pass -enable-kvm to qemu (default yes)
#enable_kvm = yes
# Explicitly pass -no-kvm to qemu (default no)
#disable_kvm = no
# For enabling an accelerator explicitly (default ''). This option has
# higher priority than `enable_kvm` or `disable_kvm`.
#vm_accelerator =
# Explicitly pass -disable-shutdown to qemu (default no)
#disable_shutdown = no
# Set the "-compat" setting option (uncomment this to crash on the use of
# a deprecated interface)
# qemu_compat = deprecated-input=crash
# Pass a custom bios path for debugging
#bios_path = /path/to/coreboot.rom

# List of virtual machine object names (whitespace separated)
vms = avocado-vt-vm1
# Default virtual machine to use, when not specified by test.
main_vm = avocado-vt-vm1

# Always set optional parameters (addr, bus, ...)
strict_mode = no
# Uncomment this to always wait 1s before executing QMP command
# (due of bug immediate use of QMP monitor after qemu start causes qemu crash)
# workaround_qemu_qmp_crash = always

# List of default network device object names (whitespace separated)
# All VMs get these by default, unless specific vm name references
# are used, for example:
#
# vms = vm1 vm2 vm3 vm4
# nics = nic1 nic2 nic3
# nics_vm1 = nic2
# nics_vm2 =
# nics_vm3 = nic1 nic3
#
# Translates to:
# VM 'vm1' uses only 'nic2'
# VM 'vm2' uses no nics
# VM 'vm3' uses 'nic1' and 'nic3'
# VM 'vm4' uses all three nics
nics = nic1
# Assign random MAC by default, or specify per-nic values
# mac_nic1 = 01:02:03:04:05:06
# Use builtin dhcpd by default, or assign per-nic IP addresses
# ip_nic1 = 192.168.122.100
# Global defauot NIC Destination type (network, bridge (tap), user)
# Add '_<nic_name>' postfix for heterogenious mapping (nettype_nic1 = bridge)

# Global default type and destination name (eth1, virbr0, default, etc.)
# Add '_<nic_name>' postfix for heterogenious mapping (netdst_nic1 = virbr0)
# Note: This setting will be overridden BY DEFAULT if using the test runner.
nettype = user
netdst = virbr0

# For private bridge created by framework please set the following
# parameters:
# netdst = private
# physical_nic(optinal): physical nic for bridge
# bridge_force_create(optinal): force to setup bridge or not, default
#                                value is no.

# Heterogenious models are also possible (default in guest-hw.cfg)
# nic_model_nic1 = e1000
# nic_model_nic2 = virtio
# or based on vm
# nic_model_vm1 = virtio
#Host IP address to be used by tests mainly in case of
#advanced network configuration of the host
#host_ip_addr = ""

# For enabling virt queues in tests uncomment below two params and
# modify based on need, net_driver value can either be vhost or qemu
# vhost - use host kernel, qemu - userspace, bydefault it is vhost if
# kernel supports
# queues = 10
# net_driver = vhost

# Set this parameter to 'yes' inorder to get IP from
# any network interface incase of multiple interface
# present in VM, default is 'no'
flexible_nic_index = no

# Add device intel-iommu
# intel_iommu = yes
# Set intel-iommu options
# iommu_intremap = on/off/auto  (on by default)
# iommu_device_iotlb = on/off  (on by default)
# iommu_caching_mode = on/off
# iommu_eim = on/off/auto
# iommu_x_buggy_eim = on/off
# iommu_version = <uint32>
# iommu_x_scalable_mode = on/off
# iommu_dma_drain = on/off
# iommu_pt = on/off
# iommu_aw_bits = <uint8>

# Enable guest iommu (intel_iommu=on) in guest kernel cmd line
# This parameter only support intel platform so the default
# value is set to None.
# enable_guest_iommu = no
# Set guest kernel iommu option (iommu=off/force/pt e.g.)
# guest_iommu_option = pt

# List of block device object names (whitespace separated)
images = image1
# List of block device object names with order (whitespace separated)
# Example: $base $sn1 $sn2
image_chain = ""
# List of optical device object names
cdroms = cd1

# To enable saving __all__ images on test failure use:
# save_image_on_error = yes
# You can also save only specified (image1) images on failure using:
# save_image_image1_on_error = yes

# USB controller object names (whitespace separated)
usbs = usb1
# USB controller type, run following command to see supported controller.
# `qemu-kvm -device \? 2>&1 | grep "usb.*bus PCI"`
usb_type = ich9-usb-ehci1
usb_type_usb1 = ich9-usb-ehci1
# Max ports on a controller.
usb_max_port = 6

# USB device object names (whitespace separated)
usb_devices = tablet1
# USB device type, run following command to see device list on your host.
# `qemu-kvm -device \? 2>&1 | grep "bus USB"`
usb_type_tablet1 = usb-tablet
# USB Controller type which device uses.
usb_controller = ehci
usb_bus = usb1.0
#use xhci as the default controller on ppc platform.
pseries:
    usb_controller = xhci
    usb_type = nec-usb-xhci
    usb_type_usb1 = nec-usb-xhci
#use qemu-xchi as the default controller for q35 and aarch64 platforms.
q35, arm64-pci, arm64-mmio:
    usb_controller = xhci
    usb_type = qemu-xhci
    usb_type_usb1 = qemu-xhci

# Serial port support
# You can assign more than one serial to guest with this parameter.
# like:
# serials = "serial0 vs1 vs2 vc1"
# serial_type_vs1 = virtserialport
# serial_type_vc1 = virtconsole
# Chardev backend type: 'unix_socket' by default
# chardev_backend_vs1 = unix_socket
# chardev_backend_vc1 = tcp_socket
# Chardev host for tcp/udp socket backend
# chardev_host = $host_ip (127.0.0.1 by default)
# chardev_ipv4 = yes
# chardev_ipv6 = yes
# chardev_name = vdagent/smartcard/usbredir (only for spicevmc and spiceport)
# Virtio port name prefix (port index will be appended)
# virtio_port_name_prefix = "com.redhat.spice."
# By default the port name is used (vc1)
# virtio_port_name_prefix_vc1 = ""
# If there is prefix defined, will replace serial_name to
# 'prefix+(the device index of the virtio_ports list)', e.g.
# serials = "serial0 vs1"
# serial_type = "isa-serial"
# serial_type_vs1 = virtserialport
# virtio_port_name_prefix_vs1 = "com.redhat.spice."
# Then the serial_name of "vs1" will be "com.redhat.spice.0"

serials = "serial0"
serial_type = "isa-serial"

# Emulated machine type, run following command to see supported machine type.
# qemu-kvm -M ?
# machine_type = pc

# Add extra parameter for pcie-root-port. It's a global parameter for all
# pcie-root-port except the one at slot 0. It works with 'pcie_extra_root_port'
# too, multiple parameters connect with ','
# pcie_root_port_params = "hotplug=off,vvv=vvv"


##### Low-level parameters for platform, networking, block, and usb devices.

# Default number of processors for each VM
smp = 1
# Config for vcpu
#vcpu_cores = 1
#vcpu_threads = 1
#vcpu_sockets = 1
#vcpu_dies = 1
! x86_64, i386:
    vcpu_dies = INVALID

# Configure cpu mode and model
# possible values: host-model, host-passthrough, custom, default:''
# virt_cpu_mode = ''
# possible values: power8, core2duo etc, default:''
# virt_cpu_model = ''
# possible values: minimum, exact, strict, default:''
# virt_cpu_match = ''
# possible values: True or False, default: False
# virt_cpu_vendor = False

# configure guest numa nodes, it can be set to "yes"
# numa = "no"

# configure number of numa nodes in guest but numa = "yes" is pre-requisite
# numa_nodes = 2

# To have guest numa pinned with host numa but numa = "yes" is pre-requisite
# numa_pin = "no"

# numa memory mode is either 'interleave', 'strict', or 'preferred',
# defaults to 'strict', numa = "yes" and numa_pin = "yes" is pre-requisite
# numa_pin_mode = "strict"

# To pin the guest numa with specific host numa, host numa node number
# can be configured, numa = "yes" and numa_pin = "yes" is pre-requisite
# pin_to_host_numa_node = 1

# Define initiator for numa node, it has to be a node with cpu, and if current
# numa node has cpus the initiator has to be itself.
# numa_initiator_node0 = 0

# Memory for each VM
mem = 1024
# Pattern for check memory size inside guest
mem_chk_re_str = ([0-9]+)

# To configure guest backed by hugepages, set it to "yes"
# hugepage = "no"

# To have guest numa nodes backed by hugepages, but numa = "yes" and hugepage =
# "yes" is pre-requisite
# hugepage_pin = "no"

# specific guest numa node number can be configured to be backed by hugepage,
# numa = "yes" and hugepage = "yes" is pre-requisite
# hugepage_pinned_numa = "0"

# Instructs hypervisor to disable shared pages (memory merge, KSM) for
# this domain, hugepage = "yes" is pre-requisite
# hp_nosharepages = "no"

# Set hugepages on specific host numa node
# target_nodes = 0 1 2

# Number of hugepages to be set on host node
# target_num_node0 = 1024
# target_num_node1 = 1024
# target_num_node2 = 1024

# Define '-numa cpu' device, for every cpu 'numa_cpu_nodeid_cpu*' is mandatory,
# optional options are socketid, dieid, coreid and threadid.
# guest_numa_cpus = "cpu0 cpu1"
# numa_cpu_nodeid_cpu0 = **
# numa_cpu_socketid_cpu0 = **
# numa_cpu_dieid_cpu0 = **
# numa_cpu_coreid_cpu0 = **
# numa_cpu_threadid_cpu0 = **

# Define '-numa dist' for the given source node, the value is a list contains
# pairs of destination node id and NUMA distance from source to destination
# numa_dist_node0 = [[1, 30], [2, 40]]

# Define '-numa hmat-lb' for the given initiator and target node,
# It can be defined with different hierarchies and data-types to certain node:
# initiator=node,target=node,
# hierarchy=memory|first-level|second-level|third-level,
# data-type=access-latency|read-latency|write-latency
# [,latency=lat][,bandwidth=bw]
# numa_hmat_lb_node0 = 'hmat_lb0 hmat_lb1'
# numa_hmat_lb_hierarchy_hmat_lb0 = 'first-level'
# numa_hmat_lb_data_type_hmat_lb0 = 'read-latency'
# numa_hmat_lb_latency_hmat_lb0 = 5
# numa_hmat_lb_hierarchy_hmat_lb1 = 'first-level'
# numa_hmat_lb_data_type_hmat_lb1 = 'read-bandwidth'
# numa_hmat_lb_bandwidth_hmat_lb1 = '100M'

# Define '-numa hmat-cache' for the given node, it can be defined with
# different levels to certain node:
# node-id=node,size=size,level=level[,associativity=none|direct|complex]
# [,policy=none|write-back|write-through][,line=size]
# numa_hmat_caches_node0 = 'hmat_cache0 hmat_cache1'
# numa_hmat_caches_size_hmat_cache0 = 30K
# numa_hmat_caches_level_hmat_cache0 = 1
# numa_hmat_caches_associativity_hmat_cache0 = direct
# numa_hmat_caches_policy_hmat_cache0 = write-back
# numa_hmat_caches_line_hmat_cache0 = 4

# memory pages belonging to the domain will be locked in host's memory
# and the host will not be allowed to swap them out, hugepage = "yes" is
# pre-requisite
# hp_locked = "no"

# Host networking restart command line for PCI assign cases
# net_restart_cmd = /etc/init.d/network restart

# Size of file-based qcow2 image, if image_raw-device not used.
image_size = 10G

# Memballoon model
memballoon_model = 'virtio'

# Use raw block device node or file-based qcow2 format
image_raw_device = no
# Name of image, or path to device node if using image_raw_device
image_name = image
# Verify whether guest boot from Hard disk.
image_verify_bootable = yes
# Pattern for verifying whether boot up from Hard disk.
image_unbootable_pattern = "Hard Disk.*not a bootable disk"

# Host-side caching to use. This option is passed directly to qemu(-kvm) so
# any valid qemu cache option is valid here. A quick reference for the
# valid cache modes on current qemu:
#
# none: avoids the page cache completely, attempt to do disk IO directly to the
#       guest memory.
#
# writethrough: use host page cache, report data as written to guest only when
#               it's actually written by the storage subsystem (qemu default).
#
# writeback: use host page cache, report data as written to guest as soon as it
#            is written to the host page cache.
#
# unsafe: use page cache, but never worry about flushing that data to disk.
#
# directsync: avoid page cache, report data as written to guest after it's
#             reported as written by the storage subsystem.
#
#drive_cache = none

# Iscsi support related params. Please fill them depends on your environment.
# For both iscsi and emulated iscsi should have this option:
# target: target name of your iscsi device
# device_id(optinal): The number of your device if the iscsi device already formatted
# iscsi_init_timeout(optional): Timeout for OS to init iscsi device under /dev after login to the target. Default value is 10s
# For iscsi device only:
# portal_ip: iscsi server ip
# initiator: initiator name
# For emulated iscsi in localhost only:
# emulated_image: image name in localhost
# iscsi_backend: iscsi backend type,such as fileio,block
# Other option parameters:
# force_cleanup: if need logout and cleanup after the case. The value should
#                be yes or no
# emulated_file_remove(emulated iscsi only): If remove the file after logout

# Nfs support related params. Please fill them depends on your environment
# For both nfs and local export nfs, following parammeters should be set:
# storage_type = nfs
# nfs_mount_dir: the local dir your want to mount to
# nfs_mount_options: mount options
# For nfs you also need set this one:
# nfs_mount_src: the nfs resource you use
# For nfs export in local these options need be set:
# export_dir: the dir export to users
# nfs_mount_src: the nfs resource you use
# At least one of the above parameters should be set. And id export_dir is
# set, it will cover the value set in nfs_mount_src.
# export_ip: optional.
# export_options: optional.
# setup_local_nfs = no

#
# List of hypervisor-monitor object names (one per guest),
# used to communicate with hypervisor to control guests.
# Order cooresponds to 'vms' list above.
monitors = qmpmonitor1
chardev_backend_qmpmonitor1 = unix_socket
# hmp1 monitor type (protocol), if only hmp1 type is going to be set
# monitor_type_hmp1 = human
# Default monitor type (protocol), if multiple types to be used
monitor_type = qmp
# If set catch_monitor, will start another monitor in qemu for
# VmRegister and ScreenDump threads.
catch_monitor = catch_monitor
chardev_backend_catch_monitor = unix_socket
#monitor_type_catch_monitor = qmp

# Add a debugging console to guest.
enable_debugcon = yes

# Guest Display type (vnc, sdl, spice, or nographic)
display = vnc

# VNC autoport selection.
vnc_autoport = yes

# Guest VGA type (cirrus,  std, vmware, qxl, xenfb, none)
vga = std
# Whether use '-vga' expression to represent the VGA device,
# if no, using '-device' expression.
vga_use_legacy_expression = no

# Add a sga device to guest.
#enable_sga = yes

# Specify multiple sound cards separated by comma,
# like 'sb16,adlib,ac97,es1370,hda' (multiple occurrences are allowed) or 'all' (-soundhw all).
# By default, it's 'ac97'.
#soundcards = ac97

###
# Capture contents of display during each test
take_regular_screendumps = yes
keep_screendumps_on_error = yes
keep_screendumps = yes
screendump_delay = 5
# Encode video from vm screenshots
encode_video_files = yes

# Record user defined vm moniter info during each test in regular interval.
# To enable multiple info cmds just use comma separated cmds
# E:g:- vm_info_cmds = "registers,cpus,pic"
# let's keep default value as "registers"
# Note: It works for both vm_types(qemu, libvirt)
vm_info_delay = 5
store_vm_info = yes
vm_info_cmds = "registers"

# Set boot order
boot_order = cdn
# Use the first hard disk for boot device once
boot_once = c
# Enable/Disable boot menu
boot_menu = off
# Enable/Disable strict boot
boot_strict = off

# Guest RTC base (utc, localtime, <datetime>)
rtc_base = utc
# Guest RTC clock (host, rt, vm)
rtc_clock = host
# Guest RTC drift (none, slew)
#rtc_drift =

#### SPICE related options valid if display == spice,
#### you should set vga = qxl to get SPICE in use
#qxl_dev_nr = 1
#qxl_vgamem_mb = 8
#qxl_vram_size = 33554432
#qxl_ram_size = 33554432
#spice_port = 3001
#spice_password = 123456
#spice_addr = 0

#spice_ssl = no
#spice_tls_port = 3002
#spice_tls_ciphers = DEFAULT
#spice_gen_x509 = yes

# x509_dir uses passphrase less key by default (defined in x509_secure)
# spice_x509_dir = no will enable x509_key_file, x509_cert_file and
# x509_cacert_file
#spice_x509_dir = yes

#spice_x509_prefix = /tmp/spice_x509d
#spice_x509_key_file = server-key.pem
#spice_x509_cacert_file = ca-cert.pem
#spice_x509_cert_file = server-cert.pem
#spice_x509_key_password = testPassPhrase
#spice_x509_secure = no
#spice_x509_cacert_subj = /C=CZ/L=BRNO/O=SPICE/CN=my CA
#spice_x509_server_subj = /C=CZ/L=BRNO/O=SPICE/CN=my Server
#spice_secure_channels = main, inputs

# Less common options
# spice seamless migration (on, off), off is semi-seamless migration
# spice_seamless_migration = on
# image compression opts (auto_glz, auto_lz, quic, glz, lz, off)
#spice_image_compression = auto_glz
# jpeg wan compression opts (auto, never, always)
#spice_jpeg_wan_compression = auto
# zlib-glz wan compression opts (auto, never, always)
#spice_zlib_glz_wan_compression = auto
# streaming-video opts (off, all, filter)
#spice_streaming_video = all
# agent mouse opts (on, off)
#spice_agent_mouse = on
# playback compression opts (on, off)
#spice_playback_compression = on
#spice_ipv4 = yes
#spice_ipv6 = no

##### Less-common and default parameters expected by some tests,
##### do not modify unless you know what you're doing.

# Whether to run the qemu binary with malloc perturb set.
#    Off by default, set to 'yes' to enable
malloc_perturb = no

# Monitor network traffic during testing
run_tcpdump = yes

# Block devices
drive_index_image1 = 0
drive_index_cd1 = 1
# You can specify a blkdebug file here, relative to kvm/blkdebug dir
#    we have a premade default.conf in there. Important to note that you
#    can set this for any image defined in the config at a given time
#    drive_blkdebug_image1 = blkdebug/default.conf
#    drive_blkdebug_image1 =
# What to do whether a read error is detected, such as 'stop'
drive_rerror_image1 =
# What to do whether a write error is detected, such as 'stop'
drive_werror_image1 =
# Remove image after test finished
remove_image_image1 = no
# KVM qcow2 image verification and backup settings
# Enable backup_image = yes only in some specific tests, such as
#    unattended_install. In all other tests, it should be no, so keep the global
#    as is.
backup_image = no
backup_dir = images/
# Enable backup_image_on_check_error = yes globally to allow isolate bad images
#    for investigation purposes
backup_image_on_check_error = no
# Enable restore_image_on_check_error = yes globally to allow restore a image
#    that had a check image with a pristine image. Works only if
#    backup_image_on_check_error = yes, of course. For tests such as
#    unattended_install, you'll have to set it to no, since if it fails, we won't
#    have an image at all, so you save time.
restore_image_on_check_error = no
# Enable restore_image = yes globally unconditionally to restore image between
#    tests. Used when you want to be *extra* careful that you're starting with
#    a fully clean and pristine image.
restore_image = no
# skip_image_processing: if yes, don't do any image processing before or
# after the test runs (corruption checking, etc.)
skip_image_processing = no
# If yes will skip the image check if vm is running even image_check is set to yes.
skip_image_check_during_running = no
# skip cluster leak warning message in image check
skip_cluster_leak_warn = no

# Create libvirt multi vms
# To enable multi libvirt vms setup, make create_vm_libvirt="yes" in addition
# to vms = vm1 vm2 vm3 and make sure `master_images_clone = img1` not be None
# inorder to get three vms cloned, provided main_vm = vm1
create_vm_libvirt = no
# Some preprocessor params
# If there is any conflict between 'start_vm' and 'kill_vm_before_test',
# the final desicion is made by start_vm.
start_vm = yes
kill_vm_before_test = no
paused_after_start_vm = no

# Some postprocessor params
kill_vm = no
kill_vm_gracefully = yes
kill_unresponsive_vms = yes
# Wait time before kill vm
kill_timeout = 60

# Undefines vm from libvirt environment if set
kill_vm_libvirt = no

# VM Undefine options during postprocess if kill_vm_libvirt is enabled
kill_vm_libvirt_options = '--managed-save'

# Cleans up the env if set
env_cleanup = no

# Verify host dmesg in postprocess.
verify_host_dmesg = yes

# Verify guest dmesg in postprocess.
verify_guest_dmesg = yes
#level_check: level of severity of issues to be checked
# 1-emerg, 2-emerg,alert, 3-emerg,alert,crit, etc
guest_dmesg_level = 3
# Whether to fail the testcase, default is to fail incase of error
guest_dmesg_ignore = False

# Whether to dump guest dmesg output to console
guest_dmesg_dump_console = no

# Screendump thread params
convert_ppm_files_to_png = no
keep_ppm_files = no
keep_ppm_files_on_error = no
screendump_quality = 30
screendump_temp_dir = /dev/shm
screendump_verbose = no
keep_video_files = yes
keep_video_files_on_error = yes

# Default remote shell port (SSH under linux)
shell_port = 22
# If you need more ports to be available for comm between host and guest,
# please see https://github.com/autotest/autotest/wiki/KVMAutotest-Networking

# Default scheduler params
used_cpus = 1
used_mem = 512

# Cpu model params
auto_cpu_model = "yes"
cpu_model_flags = ""

# Port redirections
redirs = remote_shell
guest_port_remote_shell = 22

# Profilers
profilers = kvm_stat
Ubuntu:
    profilers = perf

# Timeouts
login_timeout = 360
test_timeout = 14400

# libvirt (virt-install optional arguments)
# TODO: Rename these with 'libvirt_' prefix
use_autostart = no
use_check_cpu = yes
use_debug = yes
use_no_reboot = no
use_os_variant = no
use_os_type = yes
# if using 'url = auto' to install, url_auto_ip must match IP on
# selected virsh network or bridge
# By default, avocado-vt will try to auto detect the virbr0 IP,
# if for some reason that doesn't work you can try to set this one
# to what ifconfig tells your virbr0 IP is.
url_auto_ip =
# wait in minutes for virt-install to finish (bz still open)
use_virt_install_wait = no
virt_install_wait_time = 300

# libvirt (host information for remote testcases)
# TODO: Create auto-login between remote and local host.
local_ip = ENTER.YOUR.LOCAL.EXAMPLE.COM
local_pwd = ""
remote_ip = ENTER.YOUR.REMOTE.EXAMPLE.COM
remote_user = root
# Default password is same as local_pwd
remote_pwd = "${local_pwd}"

##### migration use only parameters

# Enable this param to "yes" to perform migration setup required
# in destination host
migration_setup = "no"

##### host information for destination and source
migrate_source_host = ENTER.YOUR.SOURCE.EXAMPLE.COM
migrate_source_pwd = PASSWORD.SOURCE.EXAMPLE
# Remove comment if local and remote host can be used for migration
# migrate_source_host = "${local_ip}"
# migrate_source_pwd = "${local_pwd}"
migrate_dest_host = ENTER.YOUR.DEST.EXAMPLE.COM
migrate_dest_pwd = PASSWORD.DEST.EXAMPLE
# migrate_dest_host = "${remote_ip}"
# migrate_dest_pwd = "${remote_pwd}"

# Migration host cn is required for TLS related env setup 
# migrate_source_host_cn = ENTER.YOUR.MIGRATE_SOURCE_CN.EXAMPLE.COM
# migrate_dest_host_cn = ENTER.YOUR.MIGRATE_DEST_CN.EXAMPLE.COM

# Please provide a shared VM image for migration
# It could be put into NFS or SAN, it is used to create
# a vm with shared_storage on source and destination
migrate_shared_storage = SHARED_IMAGE.EXAMPLE
# Additional VMs that can be used for migration
# ("vms" are added on-the-fly so ${main_vm} is already there)
# For example: "migrate_vm1 migrate_vm2 migrate_vm3"
# So please put images of vms into NFS or SAN
migrate_vms = ""
# A default vm for migration, it should be one of migrate_vms
migrate_main_vm = "${main_vm}"
# Used for loading stress for host during stress migration
migrate_load_vms = ""

# Protocol of migrate uri(tcp, udp)
migrate_proto = "tcp"
# Port of migration: 49152-49216
# Remember to open ports on both source and destination
migrate_port = 49152

# Whether to migrate vm back to src host
# migrate_vm_back = "no"

# Virsh migrate options
# virsh_migrate_options = '--live --p2p'

# Protocol used in migrate desturi, e.g. tcp, tls, ssh
# migrate_desturi_proto = 'tcp'

# Migration thread timeout
# migrate_thread_timeout = "900"

# NFS directory of guest images
#images_good = fileserver.foo.com:/autotest/images_good

# Regex for get peer device for a net device.
# This regex is for Fedora host (with qemu-kvm 0.15.*),
netdev_peer_re = "\s{2,}(.*?): .*?\\\s(.*?):"
# for RHEL5 host, the regex should be:
# netdev_peer_re = "\s{2,}(.*?): .*?\s{2,}(.*?):"
# for RHEL6 host, the regex should be:
# netdev_peer_re = "\s{2,}(.*?):.*?peer=(.*?)\s"

# You can set custom commands to verify kvm module and userspace version, here
# is a convenient example for RHEL and Fedora.
#kvm_ver_cmd = "modinfo kvm | grep vermagic | awk '{print $2}'"
#kvm_userspace_ver_cmd = "grep -q el5 /proc/version && rpm -q kvm || rpm -q qemu-kvm"

image_clone_command = 'cp --reflink=auto %s %s'
image_remove_command = 'rm -rf %s'

indirect_image_blacklist = "/dev/hda[\d]* /dev/sda[\d]* /dev/sg0 /dev/md0"

# Guest screen max inactive time until taking actions (log or throw errors)
inactivity_treshold = 1800

# # Default action if inactivity threshold is reached
inactivity_watcher = log

# Floppy size when unattended installing guests
# 1.44MB for Linux, but 2.88MB for Windows guest, same as the virtio-win.vfd
vfd_size = 1440k

# Default value, overridden by virtio-win.cfg
install_virtio = no

# Default value, overridden by other cfgs
cmds_installed_host = ""

# Additional user defined commandline args for virt-install commandline
# Default is empty, can be modified and used it in install/import test configs
virtinstall_extra_args = ""

# Add the parameter decide if setup host env in the test case in a loop
# For some special tests we only setup host in the first and last case as
# SRIOV related tests. The meaning of this flag is as following:
#    0(00): do nothing
#    1(01): setup env
#    2(10): cleanup env
#    3(11): setup and cleanup env
# You can set up it as the numbers marked in the above like this:
# host_setup_flag = 0

# Add params to enable qemu options
#msg_timestamp = on
#realtime_mlock = on
#keyboard_layout = en-us

# Params for enable/disable Libvirtd debug logs by default it is
# enabled and retrieve logs for fail and error tests, the logs
# are saved in default debug dir
enable_libvirtd_debug_log = "yes"
libvirtd_debug_level = "2"
libvirtd_debug_file = ""
libvirtd_log_cleanup = "yes"

#Define one flexbit whether enable split daemons feature, default is disable
enable_split_libvirtd_feature = "no"

# Add the params to attach strace to start qemu processes
#enable_strace = no
#strace_vms = ${main_vm}

#Add network proxies setting
#network_proxies = "https_proxy: https://proxy.com:8080/; ftp_proxy: ftp://proxy.com:3128/"
#Download location for uperf stress
#download_url_uperf = "https://github.com/uperf/uperf.git"
#Download type for uperf
#download_type_uperf = "git"
#Number of uperf threads to be run:
#nthreads = "40"
#Make command for uperf
#make_cmds_uperf = "autoreconf -f -i && ./configure && make install"
#Uninstall command for uperf
#uninstall_cmds_uperf = "./configure && make uninstall"
#Uperf server command to run
#uperf_server_cmd = "uperf -s"
#Uperf client command to run
#uperf_client_cmd = "uperf -m /home/%s -a"
#Protocol to be run with uperf stress:
#uperf_protocol = "tcp"
#Client profile to run for uperf tests
#client_profile_uperf = "shared/deps/uperf/iperf.xml"

# params to enable/disable sosreport for host/remote host
enable_host_sosreport = "no"
enable_remote_host_sosreport = "no"

# libvirt installer default options
rpmbuild_path = "/root/rpmbuild/"

# params "sysprep_required" & "sysprep_options": using which 
# users can provide required customization to prepare master 
# disk image, like "truncating machine-id" or any other tweaks 
# supported by virt-sysprep tool, followed by cloning process.
sysprep_required = no
sysprep_options = "--operations machine-id"

# Enable Code Coverage report
# prerequisite: qemu build test is run with --enable-gcov
#               configure option and preserve_srcdir = yes
#               and qemu build dir is available
# Enable/disable gcov for qemu, default: disable
gcov_qemu = no
# Enable/disable to reset the code coverage report
# generated/collected by previous test
gcov_qemu_reset = yes
# Additional command options for gcovr
# E:g:- "--html-details --html --exclude-directories=capstone"
# above options will be required to collect detailed html
# reports for individual source files and exclude "capstone"
# directory from code coverage report generation
# default: --html
gcov_qemu_collect_cmd_opts = "--html"
# Enable/disable to compress code coverage report
gcov_qemu_compress = no

Linux:
    # param for installing stress tool from repo
    stress_install_from_repo = "no"
    # param for stress tests
    stress_args = '--cpu 4 --io 4 --vm 2 --vm-bytes 256M'
    download_url_stress = 'stress/stress-1.0.4.tar.gz'

    # param to enable/disable sosreport for guest
    enable_guest_sosreport = "no"

# Add nvdimm device and specify the backend
# nv_backend = /tmp/nvdimm0

# List of input device object names (whitespace separated)
inputs = ""
# Type of input device (currently supports mouse, keyboard and tablet)
#input_dev_type = ""
# Bus type of input device (currently supports virtio)
#input_dev_bus_type = ""

# Enforce firewalld status enable/disable/none (none = don't change setting)
# firewalld_service = none
# firewalld_dhcp_workaround tweaks firewalld setting to allow host-guest
# communication. Usually this should not be necessary but there are known
# systems that require this extra setting.
Host_Ubuntu.m18.u10:
    firewalld_dhcp_workaround = yes

# Add vsock device
# vsocks = vhost_vsock0

# These parameters determine how a gluster server is used in the pool tests.
# A. If the gluster server is to be run locally on the test host, don't change
#    the gluster server access parameters (gluster_server_X and gluster_identity_file)
# B. If the gluster server is only to be used, e.g. in a pool, but not set or cleaned up
#    by the test, set 'gluster_managed_by_test = "no"'.
#gluster_server_ip = ""
#gluster_server_user = ""
#gluster_server_pwd = ""
#gluster_identity_file = ""
gluster_managed_by_test = "yes"

# Add cputune/vcpupin to guest during import/install below param
# can be uncommented and modified to match vm vcpu numbers,
# below value indicates vcpu0 will be pinned to host cpus 0-2,
# vcpu1 will not be pinned, will be left default, use "N" inorder to skip any vcpu
# vcpu2 will be pinned to host cpus 1 and 3,
# further vcpus if guest have will be left as default
#
# vcpu_cputune = "0-2 N 1,3"

# temporarily disable monitor QEMU vm exit status
vm_monitor_exit_status = no

# Bug `reboot` param from the kickstart is not actually restarts
# the VM instead it shutsoff and fails the test even after successful
# installation this is temporary workaround for the test to proceed by
# setting it to "yes"
kickstart_reboot_bug = "no"

# Force go-down method on reset. Supported options are:
# qmp: use QMP RESET event (if QMP available, usually safest option but
#      fails when VM object is being replaced (eg. migration))
force_reset_go_down_check = "qmp"

# Add -S  to qemu by default, if you don't need it, pls set it off.
qemu_stop = on

# Do not add --preconfig to qemu by default, set it on if needed.
qemu_preconfig = off

# Set uuid property of nvdimm device (Since QEMU-5.0), it supports the string
# representation of a UUID as a URN. // RFC 4122
# If you use a wrong format, will generate a random UUID base on uuid5 and
# device name.
# nvdimm_uuid = urn:uuid:ea0ec480-6066-40bc-9709-65d0467ad024
nvdimm_uuid = ""


# Set below params to attach TPM device to
# guest xml and qemu command line
# tpm_device_path takes the TPM device path in host
# tpm_model takes model of TPM device to added
# E:g models:- tpm-tis, tpm-crb, tpm-spapr etc
# tpm_type takes type of device add either
# passthrough or emulator
#
# tpm_device_path = "/dev/tpm0"
# tpm_model = "tpm-spapr"
# tpm_type = "passthrough"
# tpm_version = "1.2" # only for qemu command line


# Add extra params for dimm devices
#dimm_extra_params = "label-size=128k slot=1"

# Set below params to enable io throttle feature.
# throttle_groups indicates throttle group object,
# It may have multi groups separated by spaces.
# throttle_group_parameters indicates corresponding option for the group,
# It uses dictionary data struct to describe the property.
# image_throttle_group indicates the image belong to which group.
# Example:
# Declare two throttle groups
# throttle_groups = " group1 group2"
# Declare specific throttle group properties
# throttle_group_parameters_group1 =  {"iops-total":40}
# throttle_group_parameters_group2 =  {"iops-total":50,"bps-total":1024000}
# Declare image stg1 belong to throttle group1
# image_throttle_group_stg1 = "group1"
# Declare image stg2 belong to throttle group2
# image_throttle_group_stg2 = "group2"

# Update guest default kernel option via serial session
# kernel_extra_params_serial_login = no

#Set below params to enable cor filter feature.
#image_copy_on_read: 'yes', 'on' or 'true' to make a cor filter insert enabled.
#COR filter can be insterted  above any format node in the snapshot chain
#Data in the backing chain below this COR filter will be copied when read.
#COR filter can be placed on the top or on the base of the snapshot chain,
#and paricipate in stream/commit operations.
#Example to use COR filter:
#-blockdev node-name=qcow2_data1,driver=qcow2,read-only=off,cache.direct=on,cache.no-flush=off,file=file_data1 \
#-blockdev node-name=drive_data1,driver=copy-on-read,file=qcow2_data1 \
